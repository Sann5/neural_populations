# notes about GMM's
* reasoning about the distribution over inputs give us a handle over reasoning about missing labels.
* we assume that out data is generated by a Gaussian given a specific label.
* A gaussian mixture is a linear conbination of Gaussian distributions. $\theta$ is all the parameters in out model. \
$$
P(x|\theta ) = \sum_z P(x, z | \theta ) = \sum_z P(z | \theta ) P(x | z, \theta )
$$

* One could perform MLE on the likelihood which is given by the $\log P(x|\theta)$, but its hard to solve (non-convex). Still however can be done with gradeint decent methods. EM algo is an alternative to using a gradeint decent method.
* one can think of clustering as latent variable modeling, which is what we are doing with GMM's
* in practice hard EM may perform poorly if clusters are overlapping. perform poorly in the sense of not recovering the parameter of the underlying distribution which is, in the context of WTL-kmeans the design of the inputs. but perhaps this is better since we would like to spread out our neurons, so each is more tuned to a sepcific part od the input design? or rather we would like to have spciallized neurons that are the summerize of a specific more broadly a specific input? is our goal to obtain some population level activity structure given a specifi input pattern?
* the EM algorithm generalizes to more general family of distributions (the exponentials) because the complete dat alikelihood can be fully computed with the expected suficient statistics (cluster memberships in this case).
* GMM's can capture more complex cluster structures. 
* as variances shrink the membership probabilitiesget more and more polarizeds, tending to the hard assignemet. 
* interestingly? cross validation works well for GMM's, if data is trully generated from a GMM.

### Degeneracy
* Its easy for the algo to shrink the variance to zero to increase the probability of a point belonging to a certain cluster, but this might lead to overfitting.
* one way to avoid this is by setting al lower bound on the variance
* another way is to add a regularizor, which is adds some varinace to the diagonal of $\Sigma_j = \Sigma_j + \mu I$. This can be seen as placing a Wishart prior on trhe covariacne matrix and computing the MAP estimate instead of the MLE during the M step update. makes the variance of the underlyng clusters bigger
* there can aslo be problems when the data livea on a low dimensional subspace (lots of correlartion between differen features), degerenacy, numerical problems.

### Interesting ideas
* its an interesting idea that sometimes we refer to data points as belogning to clusters, or at least with a certain probability inthe context of the soft EM algo. maybe we could aslo think of inputs to belonging to certain neurons, or at least being tuned to them. 
* accounting for weight diference between clusters allowes us to model the probability that a specific observation belongs to a secific class (e.g. cluster size). **Maybe this is a way of naturally modeling homeostasis.** Prior infomration about the size is hard to use to intialize the algorithm since you also need information about hteir location the the data space. Is propbability of input belonging to a certain neuron = homeostasis activity?
* a gaussian mixtrue for each class instead of a single gaussina component
* to use this model for classification one can compute the probability of the input belongin to a class using bayes rule. 
* one can also use the fitting of a GMM for density estimation, ege detecting outliers. 
* anommaly detection is something that the human brain is good at doing? might it also be doing it through a generative modeling apprach? not clear how since an anomalous input would just exite the network in an unexpected way (low activity?). how is expected/unexpected encoded?

### Semi-supervised learning
* not really applicable sin in the brain we only have the memory encoded in the weights and not a datapoint and its label stored somewhere

### Theoretical properties of the EM
* convergence proof and theoretical properties minute 50, class 18.05.2021 (GMM's II)
* Garanteed to monotonically increase the likelihood. 
* solution depends on initialization since the maximization found is only local

## GAN's
* generating data. take a simple (for example gaussian) RV and send it thorugh a NN, in order to obtain some non linear version of Z.
* for these type of models its dificult to evaluate the likelihood of the resulting non linear distribution of the transformed version of Z. 
* likelihood free training criteria. 
* two common approaches: variational encoders and generative adversarial neural networks

