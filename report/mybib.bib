@article{Moraitis2021,
  author    = {Timoleon Moraitis and
               Dmitry Toichkin and
               Yansong Chua and
               Qinghai Guo},
  title     = {SoftHebb: Bayesian inference in unsupervised Hebbian soft winner-take-all
               networks},
  journal   = {CoRR},
  volume    = {abs/2107.05747},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.05747},
  eprinttype = {arXiv},
  eprint    = {2107.05747},
  timestamp = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-05747.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Keck2012,
    doi = {10.1371/journal.pcbi.1002432},
    author = {Keck, Christian AND Savin, Cristina AND Lücke, Jörg},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Feedforward Inhibition and Synaptic Scaling – Two Sides of the Same Coin?},
    year = {2012},
    month = {03},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pcbi.1002432},
    pages = {1-15},
    abstract = {Feedforward inhibition and synaptic scaling are important adaptive processes that control the total input a neuron can receive from its afferents. While often studied in isolation, the two have been reported to co-occur in various brain regions. The functional implications of their interactions remain unclear, however. Based on a probabilistic modeling approach, we show here that fast feedforward inhibition and synaptic scaling interact synergistically during unsupervised learning. In technical terms, we model the input to a neural circuit using a normalized mixture model with Poisson noise. We demonstrate analytically and numerically that, in the presence of lateral inhibition introducing competition between different neurons, Hebbian plasticity and synaptic scaling approximate the optimal maximum likelihood solutions for this model. Our results suggest that, beyond its conventional use as a mechanism to remove undesired pattern variations, input normalization can make typical neural interaction and learning rules optimal on the stimulus subspace defined through feedforward inhibition. Furthermore, learning within this subspace is more efficient in practice, as it helps avoid locally optimal solutions. Our results suggest a close connection between feedforward inhibition and synaptic scaling which may have important functional implications for general cortical processing.},
    number = {3},

}

@misc{Mixture_model,
   author = "Wikipedia",
   title = "{Mixture model} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2022",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Mixture\%20model&oldid=1082352648}},
   note = "[Online; accessed 28-June-2022]"
 }
 
@misc{malone_2014, 
	place={Helsinki}, 
	title={Expectation-Maximization for Estimating Parameters for a Mixture of Poissons}, 			publisher={Department of Computer Science, University of Helsinki}, 
	author={Malone, Brandon}, 
	year={2014}, month={Feb}
} 

@misc{piech_2018, title={Gradient Ascent}, publisher={Stanford University}, author={Piech, Chris}, year={2018}, month={Nov}
} 

@article{inouye2017,
author = {Inouye, David I. and Yang, Eunho and Allen, Genevera I. and Ravikumar, Pradeep},
title = {A review of multivariate distributions for count data derived from the Poisson distribution},
journal = {WIREs Computational Statistics},
volume = {9},
number = {3},
pages = {e1398},
keywords = {Poisson, Multivariate, Graphical models, Copulas, High dimensional},
doi = {https://doi.org/10.1002/wics.1398},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wics.1398},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1398},
abstract = {The Poisson distribution has been widely studied and used for modeling univariate count-valued data. However, multivariate generalizations of the Poisson distribution that permit dependencies have been far less popular. Yet, real-world, high-dimensional, count-valued data found in word counts, genomics, and crime statistics, for example, exhibit rich dependencies and motivate the need for multivariate distributions that can appropriately model this data. We review multivariate distributions derived from the univariate Poisson, categorizing these models into three main classes: (1) where the marginal distributions are Poisson, (2) where the joint distribution is a mixture of independent multivariate Poisson distributions, and (3) where the node-conditional distributions are derived from the Poisson. We discuss the development of multiple instances of these classes and compare the models in terms of interpretability and theory. Then, we empirically compare multiple models from each class on three real-world datasets that have varying data characteristics from different domains, namely traffic accident data, biological next generation sequencing data, and text data. These empirical experiments develop intuition about the comparative advantages and disadvantages of each class of multivariate distribution that was derived from the Poisson. Finally, we suggest new research directions as explored in the subsequent Discussion section. WIREs Comput Stat 2017, 9:e1398. doi: 10.1002/wics.1398 This article is categorized under: Statistical and Graphical Methods of Data Analysis > Multivariate Analysis},
year = {2017}
}

@article{carandini1994,
author = {Matteo Carandini  and David J. Heeger },
title = {Summation and Division by Neurons in Primate Visual Cortex},
journal = {Science},
volume = {264},
number = {5163},
pages = {1333-1336},
year = {1994},
doi = {10.1126/science.8191289},
URL = {https://www.science.org/doi/abs/10.1126/science.8191289},
eprint = {https://www.science.org/doi/pdf/10.1126/science.8191289},
abstract = {Recordings from monkey primary visual cortex (V1) were used to test a model for the visually driven responses of simple cells. According to the model, simple cells compute a linear sum of the responses of lateral geniculate nucleus (LGN) neurons. In addition, each simple cell's linear response is divided by the pooled activity of a large number of other simple cells. The cell membrane performs both operations; synaptic currents are summed and then divided by the total membrane conductance. Current and conductance are decoupled (by a complementary arrangement of excitation and inhibition) so that current depends only on the LGN inputs and conductance depends only on the cortical inputs. Closed form expressions were derived for fitting and interpreting physiological data. The model accurately predicted responses to drifting grating stimuli of various contrasts, orientations, and spatiotemporal frequencies.}
}


@article{carandini2012,
	abstract = {Normalization computes a ratio between the response of an individual neuron and the summed activity of a pool of neurons.The normalization model was developed to explain responses in the primary visual cortex (V1), and has been seen to operate in a variety of other regions of the visual system: light adaptation in the retina, contrast normalization in the retina and lateral geniculate nucleus, and visual processing in higher visual cortical areas beyond V1.Normalization has also been proposed to be at the root of the modulatory effects of visual attention on neural responses in the visual cortex.Normalization is seen in multiple species and brain regions. These include olfactory processing and representation in the fruitfly antennal lobe, the encoding of value in the posterior parietal cortex, multisensory integration of visual motion and vestibular signals, and auditory processing in the primary auditory cortex.Different (feedforward and feedback) neural circuits and mechanisms might perform normalization, including presynaptic inhibition, shunting inhibition, synaptic depression, changes in the amplitude of ongoing activity and balanced amplification.The effects of normalization can be measured behaviourally.The computational benefits of normalization include maximizing sensitivity, providing invariance with respect to some stimulus dimensions at the expense of others, facilitating the decoding of a distributed neural representation, facilitating the discrimination among representations of different stimuli, providing max-pooling (winner-take-all competition) and reducing redundancy.Understanding canonical neural computations such as normalization may shed light on psychiatric, neurological and developmental disorders.},
	author = {Carandini, Matteo and Heeger, David J.},
	date = {2012/01/01},
	date-added = {2022-06-29 18:09:17 +0200},
	date-modified = {2022-06-29 18:09:49 +0200},
	doi = {10.1038/nrn3136},
	id = {Carandini2012},
	isbn = {1471-0048},
	journal = {Nature Reviews Neuroscience},
	number = {1},
	pages = {51--62},
	read = {0},
	title = {Normalization as a canonical neural computation},
	url = {https://doi.org/10.1038/nrn3136},
	volume = {13},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1038/nrn3136}
}


@article{pouille2009,
	abstract = {The cortex is sensitive to weak stimuli, but also responds to stronger inputs without saturating. In this study, Scanziani and colleagues find some of the circuits that enable neuronal populations to respond to a wide range of input strengths.},
	author = {Pouille, Fr{\'e}d{\'e}ric and Marin-Burgin, Antonia and Adesnik, Hillel and Atallah, Bassam V and Scanziani, Massimo},
	date = {2009/12/01},
	date-added = {2022-06-29 18:11:53 +0200},
	date-modified = {2022-06-29 18:12:12 +0200},
	doi = {10.1038/nn.2441},
	id = {Pouille2009},
	isbn = {1546-1726},
	journal = {Nature Neuroscience},
	number = {12},
	pages = {1577--1585},
	read = {1},
	title = {Input normalization by global feedforward inhibition expands cortical dynamic range},
	url = {https://doi.org/10.1038/nn.2441},
	volume = {12},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1038/nn.2441}
}
