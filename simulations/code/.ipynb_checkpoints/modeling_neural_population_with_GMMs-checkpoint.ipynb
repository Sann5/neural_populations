{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Neural Population with Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "TODO: redo all of this. Touch on:\n",
    "\n",
    "- Why we started this. Problem with unupdated centroids.\n",
    "- Wanted to incormporate homeostasis. \n",
    "\n",
    "The present work explores how one can model learning in neural populations as an insatnce of online Gaussian Mixture Model (GMM) learning. First we descirbe the Hard Expectation-Maximization (EM) algorithm, a common method to infere GMM's. We then show how by assuming sperichal varainces and uniform wights the Hard EM algorithm becomes one and the same with the K-means algorithm. Then we show how K-means can be seen as an istance of a Winner Takes it All (WTA) artificial neural network (ANN). \n",
    "\n",
    "We then briefly describe why the EM algorithm is convenient to model the system at hand and go on to relax the simplifications made in the Hard EM algorithm to give way tho the Soft EM algorithm. Finally we touch on the implications that lifting these simplifications has on the analogue nerual model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WTA-ANNs. Artificial neural networks of biological interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Winner takes it all artificial neural networks (WTA-ANNs) have long been studied in computational neuroscience becasue they provide a simple model for a common conectivity pattern in the brain cortex, where neurons compete with each other for activation through lateral inhibition. \n",
    "\n",
    "WTA-ANNs are 2 layered, fully conected ANNs where only one output neuron $z_j$ (we will refere to any of the $K$ neurons in the output layer as neuron $z_j$) is be activated per input. Learning takes place through Hebbian adjustments of the weights $\\Lambda$ between these two layers every time an input is presented to the network. Only the weights $\\lambda_{j:}$ (the row vectors of matrix $\\Lambda$) associated with the winning output neuron $z_j$ are adjusted per input, the winning neuron being the one with weights $\\lambda_{j:}$ that are the closest, accoridng to some messure of distance, to the $n^{th}$ input vector $^nx$, however when its not important to indicate the index of the data poin $x$ will be used. We will refere to the value of the $i^{th}$ neuron in the input layer with $x_i$, having in total $M$ neurons in this layer.\n",
    "\n",
    "TODO: make poicture with appropiate symbols, aslo maybe the matrix mutliplication of the output. Perhaps pseudocode?\n",
    "\n",
    "Usually the goal of such a ANN is to optimally classify the inputs using the $K$ output neurons. The idea is that given an input pattern $^nx$, the asociated weights $\\lambda_{j:}$ of winning neuron $z_j$ gets adjusted such that, if the same or a similar input $^{n+1}x$ is presented to the network then $z_j$ is again the winner, thereby succesfully classifying the input. Its also important to note the impact of the lateral inhibition here, which makes it so only one output neuron gets activated.\n",
    "\n",
    "Even thou it is conveninet to think of all $\\lambda_{j:}$ weight associated with winning neuron $z_j$ to be updated as a whole in vector form, we can also think of the update as takeing place individually for each element of $\\lambda_{ji}$, associated with input neuron $x_i$ and output neuron $z_j$. The weight updates then become akin to biologial Hebbain learning where learning happens locally, where becasue the presynaptic neuron fires shortly before the postsynaptic neuron the synaptic weight between the two is strengthened. \n",
    "\n",
    "If we restrict the weights and the inputs to be normalized, such that the sum of the elements in the input vectors $x$ and all weight vector $\\lambda_{j:}$ are equal to a constant (TODO: tag equation), we can implement the learning by adjusting the previous weight wectors $\\lambda_{j:}$, the adjusment being function of the difference between $\\lambda_{j:}$ and input vector $x$ (TODO: tag eaquation). For mor infomration on the biological plausibiulity of normalization in biological neual circuits we refere the reader to (TODO: cite normalization papers).\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^M \\lambda_{ji} = \\alpha\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^M x_i = \\alpha\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^{(t)}_{j:} \\leftarrow \\lambda^{(t-1)}_{j:} - \\eta \\epsilon'(x - \\lambda^{(t-1)}_{j:})\n",
    "$$\n",
    "\n",
    "Here $^{(t)}$ indicates the index of the iteration in the contex of an algorithm, and $\\eta$ is a small positive learning rate.\n",
    "\n",
    "If we choose the squared L2 norm as our messure of distance $\\epsilon(x - \\lambda^{(t-1)}_{j:})$ we can take the derivative w.r.t $\\lambda_{j:}$ and use the result as the update function $\\epsilon'(x - \\lambda^{(t-1)}_{j:})$. \n",
    "\n",
    "$$\n",
    "\\epsilon(x - \\lambda^{(t-1)}_{j:}) = \\frac{1}{2} ||x - \\lambda^{(t-1)}_{j:}||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d}{d\\lambda^{(t-1)}_{j:}} \\epsilon(x - \\lambda^{(t-1)}_{j:}) =  \\frac{d}{d\\lambda^{(t-1)}_{j:}} \\frac{1}{2} ||x - \\lambda^{(t-1)}_{j:}||_2^2 = -(x - \\lambda^{(t-1)}_{j:}) = \\epsilon'(x - \\lambda_{j:})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda^{(t)}_{j:} \\leftarrow \\lambda^{(t-1)}_{j:} + \\eta (x - \\lambda^{(t-1)}_{j:})\n",
    "$$\n",
    "\n",
    "The result is an online version of the K-means algoritm, where upon an inpuit the closest centroid (here the weights $\\lambda_{j:}$), gets pulled towareds the coordiantes of the input $x$. A perfect equivalence with the K-means algorithm would require that all the inputs $^nx$ to be presented at once. In such case we would simply adjust our cost function to account for all the data points $N'$ associated to the winning neuron $z_j$. By minimizing this quantity (applying the derivative, seting it to zero and solving for $\\lambda_{j:}$) we find that the update is the same as in the K-means algorithm, showing the equivalence between these two simingly different clustering schemas. \n",
    "\n",
    "$$\n",
    "\\epsilon(X - \\lambda_{j:}) = \\frac{1}{2} \\sum_{n=1}^{N'} ||^nx - \\lambda_{j:}||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda_{j:}} = \\underset{\\lambda_{j:}}{\\operatorname{arg min}} \\frac{1}{2} \\sum_{n=1}^{N'} ||^nx - \\lambda_{j:}||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda_{j:}} = \\frac{1}{N'} \\sum_{n=1}^{N'} (^nx)\n",
    "$$ \n",
    "\n",
    "\n",
    "TODO: perhaps put the pseudo code as a figure so it doesn interrupt the text\n",
    "\n",
    "#### K-means algorithm\n",
    "##### Step 0\n",
    "Initialize parameters for each centroid. \n",
    "\n",
    "##### Step 1\n",
    "For all $N$ data points find the closes cluster and assign it membership over the data point.\n",
    "\n",
    "$$\n",
    "^nz^{(t)}_j = \\underset{\\lambda_{j:}}{\\operatorname{arg min}} \\| ^nx - \\lambda_{j:}\\|_2^2\n",
    "$$\n",
    "\n",
    "##### Step 2\n",
    "Update the $K$ centroids, with the average of the coordinates of its childern data poitns.\n",
    "\n",
    "$$\n",
    "\\lambda_{j:}^{(t+1)} = \\frac{1}{N'} \\sum_{n=1}^{N'} (^nx)\n",
    "$$\n",
    "\n",
    "##### Step 3\n",
    "Repeat step 1 and 2 until convergence\n",
    "\n",
    "The vanilla WTA-ANN as well as the K-means algorithm have the problem that becasue only the weights $\\lambda_{j:}$ asociated to the winnig neuron $z_j$ get updated, it is common for some centorids to be asigned two or more clusters of data points, while other centroids are left without \"children\" datapoints and hence never get updated. Here we call this issue uneaven membership assignment. This is partly due to the \"hardness\" of the asignment step, or in other words the fact that only one neuron can win and therefore represent a given input. By modeling centroids as being only partly responsable for a given data point, instead of belonging exlusively to one, we can somewhat bypass this problem, although not entierly since the performance of the algorithm will sitll be highly dependent on initialization.\n",
    "\n",
    "<img align=\"center\" src=\"./img/problems_kmeans.png\" width=\"400\">\n",
    "\n",
    "In the following section we porpose a porbabilistic interpretation of the learning procedure that is taking place in the WTA-ANNs. Then we use this proposed farmework to design a Soft-WTA where the ouputs are rather responsabilities than all-or-none responses. These repsponsabilities encode the probability that a speciofic neuron fires given a specific input pattern, and therefore allow for multiple output neurons to get activated upon an input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set identifies the sub-population to which an individual observation belongs. Usually mixture models are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population (TODO: [quoute this from wikipedia](https://en.wikipedia.org/wiki/Mixture_model#:~:text=In%20statistics%2C%20a%20mixture%20model,which%20an%20individual%20observation%20belongs)).\n",
    "\n",
    "Just by reading this description we can already sense that mixture models might be a good fit for modeling the classfication of patterns as in the WTA-ANN. Mixture models have latent and observed random variables (RVs). The observed RVs correspond in our case to the input patterns (e.g. for all $N$ neurons $^nx = [x_1, ..., x_M]$). The idea is that each obervation can be expressed as a linear combination of the latent mixture components, where the mixture weights specify the importance of each component in explaining a given input (TODO: cite equation below here). We can think of the latent variables as the identity of the causes that induce a specific input patern, each one having a associated distribution. Here we specify generally a mixture model as follows (note the correspondance to the previos section).\n",
    "\n",
    "* $K$ = number of mixtrue components, individual components indicated with subscript $j$. Wometimes we refere to components as clusters.\n",
    "* $N$ = number of observations, individual observations indicated with left superscript $n$, where each observation $^nx \\in \\mathbb{R}^M$ is a vector where $^nx_i$ is the $i^{th}$ element of the $n^{th}$ observation.\n",
    "* $w$ = vector of mixtrue weights (prior probability of components) where $w_j$ is the mixure weight of component $j$. Can also be expressed as the prior probability $P(z_j|\\theta)$ of a random data point belonging to any of the $K$ components. Note that $\\sum_{j=1}^k w_j = 1$ \n",
    "* $\\Lambda$ = matrix of weight coeficientes, where rows $\\lambda_{j:}$ correspond to the vector of the centroids and $\\lambda_{ji}$ to the $ji^{th}$ entrie.\n",
    "* $^nz \\in \\mathbb{R}^K$ = vecotr of probabilities where $^nz_j$ is RV representing the probability of datapoint $n$ belonging to cluster j.\n",
    "* $^{(t)}$ indicates the index of the iteration in the context of an algorithm.\n",
    "* $\\theta = \\{w, \\Lambda\\}$ is the set of all parameters\n",
    "\n",
    "The likelihood of one data point can be written as follows, where $P(x | z_j , \\theta)$ is the probability density function of the $j^{th}$ component.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x:\\theta) = \\sum_{j=1}^K P(z_j|\\theta) P(x | z_j , \\theta)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x:\\theta) = \\sum_{j=1}^K w_j P(x | z_j , \\theta)\n",
    "$$\n",
    "\n",
    "There is two main inference problems when dealing with mixture models. On one hand one would like to know the number and functional form (parametric fmaily) of the components within a mixture. This is usually referred to as the system identification. On the other hand one would like to estimate the corresponding parameter values given a fixed number of components and their functional form. This is referred to as parameter estimation. In this work we concer ourselfs with he problem of parameter estimation, since we assume that the number of components and their undelying distributions is fixed. Furthermore we take Expectation Maximization as or method of choice for inference (TODO: footnote explinng that the reason why this is is mainly convenience, beacue we can esaly show that a cosntrained form EM is equivlaent yo K-means).\n",
    "\n",
    "In the context of mixture models, the EM can be seen as unsupervised learnign algorithm. This is interesting becase if it is applicable to learning in WTA-ANNs it loosly implies that it might be useful to reason about learning in the brain as an unsupervised learnign procedure, where labels are the stimuly (the hidden RVs) which generate specific input pattern distributions (the observed RV) that the barin then categorizes as belonging to the stimuly by tuning the weights of the synapses (TODO: cite here Feedforward Inhibition and Synaptic Scaling paper). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EM algrithm\n",
    "The EM algorithm iterates until convergence. One iteration of the EM starts by performming the expectation step (E-step), where expectation values for the membership variables of each data point are computed, followed by and maximitation step, or M steop which compleats one full iteration. \n",
    "\n",
    "In the E step we seek to find the responsability $\\gamma_j(^nx)$ of each cluster on each data point $^nx$, or in other words the probability that data point $^nx$ belongs to cluster $j$.\n",
    "\n",
    "$$\n",
    "\\gamma_j(^nx) = P (z = j | ^nx, \\theta) = \\frac{P(z = j, ^nx, \\theta)}{P(^nx, \\theta)} = \\frac{P(^nx|z = j, \\theta) P(z = j | \\theta)}{P(^nx| \\theta)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(^nx|\\theta) = \\sum_k P(^nx|\\theta, z = k) P(z = k|\\theta)\n",
    "$$\n",
    "\n",
    "We know from before that $P(z = j|\\theta) = w_j$ are the prior porbailities and $P(^nx|\\theta, z = k)$ is the likelihood, which depends on the parametric family we decide on. \n",
    "\n",
    "$$\n",
    "\\gamma_j(^nx) = \\frac{w_jP(^nx|\\theta, z = k)}{\\sum_{j=1}^K w_k P(^nx|\\theta, z = k)}\n",
    "$$\n",
    "\n",
    "Once we have all the $\\gamma$'s for each data point, we again have a fully annotated data set, namely the responsabilities and the cordinates of the data points. We can now move on to infere the parametrs $\\theta$ in the M step using the previosly estimated $\\gamma$'s. For this we take the expression of the likelihood of the data given the parameters and we maximize its lower bound. The derivation is as follows.\n",
    "\n",
    "We can rewrite equation (TODO: cite above equation) accounting for all $N$ data points, applying the log and then Jensen's inequality to get the lower bound on the data likelihood. We maximize this quantitiy instead of the log likelihood directly simply beaceuse of convenience (the log likelihood is hard to evaluate).\n",
    "\n",
    "$$\n",
    "\\mathcal{l}(D:\\theta) = \\sum_{n=1}^N\\log\\sum_{j=1}^K w_j P(^nx | z_j , \\theta) \\ge \\sum_{n=1}^N \\sum_{j=1}^K \\gamma_j(^nx)\\log\\frac{w_j P(^nx | z_j , \\theta)}{\\gamma_j(^nx)} = b(\\theta)\n",
    "$$\n",
    "\n",
    "We can then expand the logarithm, distribute $\\gamma_j(^nx)$, and split the sum.\n",
    "\n",
    "$$\n",
    "b(\\theta) = \\sum_{n=1}^N \\sum_{j=1}^K \\gamma_j(^nx)\\log w_j P(^nx | z_j , \\theta) - \\sum_{n=1}^N \\sum_{j=1}^K \\gamma_j(^nx)\\log \\gamma_j(^nx)\n",
    "$$\n",
    "\n",
    "Becasue the $\\gamma_j(^nx)$'s are already fixed from the E step we can concentrate only on the first term which we term $Q(\\theta)$.\n",
    "\n",
    "$$\n",
    "Q(\\theta) = \\sum_{n=1}^N \\sum_{j=1}^K \\gamma_j(^nx)\\log w_j P(^nx | z_j , \\theta)\n",
    "$$\n",
    "\n",
    "To get an estimate for the our parameters $\\theta$ we perform the partial derivatives with respect to each $\\lambda_{ji}$ and $w_j$ make it equal to 0 and solve for the corresponding parameter. This will look diferent depending on the parametric familiy we choose and also the assumtions we make. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the following we show that by considering the components of a mixture model to be Gaussian and by further constraining it, namely by inforcing uniform varaince in all components and uniform mixture weights, we recover the K-means algorithm, which imiplies that we might be able to use the EM algorithm to device a more general procedure for learning with a WTA-ANN. This constrained version of the EM algorithm is termed the Hard EM. We then show how by easing up these restictios we obtain a similar version of K-means but with soft membership assignemt, which naturally eases the uneaven membership problem. This is called the Soft EM. Only then do we move back and try to apply the probabilistic framework sorrounding the EM algorithm to our WTA-ANN framework.\n",
    "\n",
    "For the Hard EM the E step classifies each data point as belonging to only one of the clusters. We denote this correspondance as $^nz_j$, where $n$ indicates the index of a specific data point. In the following equations superscript $(t)$ indicates the iteration in the algoritm and $\\theta$ our model parameters, namely $\\mu_j$ and $\\Sigma_j$ the mean and covariance matrix of cluster $j$.\n",
    "\n",
    "### The Hard EM \n",
    "#### E-step\n",
    "$$\n",
    "^nz^{(t)} = \\underset{j}{\\operatorname{arg max}} P(z_j | ^nx, \\theta^{(t-1)})\n",
    "$$\n",
    "\n",
    "We can decompose $P(z_j | ^nx, \\theta^{(t-1)})$ using Bayes rule to get the follwoing expression.\n",
    "\n",
    "$$\n",
    "^nz^{(t)} = \\underset{j}{\\operatorname{arg max}} \\frac{1}{Z} P(z_j | \\theta^{(t-1)}) P(^nx | z_j, \\theta^{(t-1)}) \n",
    "$$\n",
    "\n",
    "$\\frac{1}{Z}$ is the normalization constant from Bayes rule which is also independen of $z$ and therefor can be omited in the maximization.\n",
    "\n",
    "$P(z | \\theta^{(t-1)})$ is the probabity of any data point belonging to cluster $z$. By assuming that each cluster has the same size, these probabilities are the same for all clusters (e.g. $\\frac{1}{K}$) and hence do not play a role in the maximization argument. $P(^nx | z, \\theta^{(t-1)})$ is the probability of our data point belonging to cluster $j$ given our past estimate of the parameteres $\\theta^{(t-1)}$, namely $\\mathcal{N}(^nx; z_j, \\theta^{(t-1)})$. These observations give way to the folowing expression.\n",
    "\n",
    "$$\n",
    "^nz^{(t)} = \\underset{j}{\\operatorname{arg max}} \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^d}} \\exp(\\frac{-1}{2\\sigma^2} \\| ^nx - \\mu_j^{(t-1)}\\|_2^2 )\n",
    "$$\n",
    "\n",
    "We can see again that the maximization is only over the squared 2-norm and we can aslo note that maximizing this quantity is equivalent to minimizing the 2-norm.\n",
    "\n",
    "$$\n",
    "^nz^{(t)} = \\underset{z}{\\operatorname{arg min}} \\| ^nx - \\mu_j^{(t-1)} \\|_2\n",
    "$$\n",
    "\n",
    "Hence the E-step is equivalent to Step 1 of K-means and in the WTA-ANN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M-step\n",
    "Thanks to the estimates of the cluster membership computed in the E-step we now have a full \"labeled\" data set $D^{(t)} = \\{ (x_1, z_1^{(t)}), \\dots, (^nx, z_n^{(t)})\\}$ and can now do maximum likelihood estimation (MLE) on a function that bounds from below the likelihood of the data.\n",
    "\n",
    "Since in this specific case we are only updating the means $\\mu_j$ and leaving both the weights and the variances untoched we only show the expresion for the fromer. \n",
    "\n",
    "$$\n",
    "\\mu_j^{(t)} = \\frac{1}{N'} \\sum_{n=1}^N' (^nx)\n",
    "$$\n",
    "\n",
    "We can now see that by considering uniform spherical variances and weights the Hard EM perfomres exactly the same operations as the K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Soft EM\n",
    "Techincially speaking one could also perform the Hard EM algorithm relaxing both the spherical covariance and fixed uniform weights asusmtion. This is equivalent to allowing the underlying cluster to have different sizes and eliptical shapes, which might already help further dilute uneaven membership assignment. However, the Hard EM will greedly assign a data point to one of the cluster even thou the probability that it belongs to either of them is the same or almost the same. This may lead to poor performance specially when the clusters are overlaping. By using membership intead of hard assignments we account for this uncertainty and help poorly intialized clusters to get updated, since even if they are far away from data points, they will still have a responsability over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E step\n",
    "For every datapoin $n$ and every cluster $j$ compute $\\gamma_j(^nx)$\n",
    "\n",
    "$$\n",
    "\\gamma_j(^nx) = \\frac{w_j\\mathcal{N}(^nx|\\theta, z = k)}{\\sum_{j=1}^K w_k \\mathcal{N}(^nx|\\theta, z = k)}\n",
    "$$\n",
    "\n",
    "#### M step\n",
    "Compute the closed form expression for the parameters $w_j, \\mu_j$ and $\\Sigma_j$, for every cluster $j$ (derivation not shown).\n",
    "\n",
    "$$\n",
    "w_j^{(t)} \\leftarrow \\frac{1}{n} \\sum_{i=1}^n \\gamma_j^{(t)}(^nx) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_j^{(t)} \\leftarrow \\frac{\\sum_{i=1}^n \\gamma_j^{(t)}(^nx)^nx}{\\sum_{i=1}^n \\gamma_j^{(t)}(^nx)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_j^{(t)} \\leftarrow \\frac{\\sum_{i=1}^n \\gamma_j^{(t)}(^nx)(^nx-\\mu_j^{(t)})(^nx-\\mu_j^{(t)})^T}{\\sum_{i=1}^n \\gamma_j^{(t)}(^nx)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online EM algorithm\n",
    "Ok so all nice and good but the EM is fromulated above for Gaussians is still for batch processing. We would like an alternative way of maximizing the lower bound on the likelihoood since in out WTA-ANN we only have observations available one at a time. The way we do this is that we try to maximize the likelihood by gradient accent (TODO: cite gradient decent).\n",
    "\n",
    "In the following we show this approach for the Gaussian and constreained Hard EM.\n",
    "\n",
    "$$\n",
    "\\mu_j^{(t)} = \\mu_j^{(t-1)} + \\eta \\frac{\\delta Q(\\mu_j^{(t-1)})}{\\delta \\mu_j^{(t-1)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q(\\theta) = \\sum_{n=1}^N \\sum_{j=1}^K \\gamma_j(^nx)\\log w_j P(^nx | z_j , \\theta)\n",
    "$$\n",
    "\n",
    "Since in the Hard EM all $\\gamma_j(^nx)$ are 0 exept for that the parent component, $w_j$ is fixed, and we only have one data poin ($N = 1$) we have that $Q(\\theta)$ is equal to:\n",
    "\n",
    "$$\n",
    "Q(\\theta) = \\log P(x | z_j , \\theta) = \\log \\mathcal{N}(x | z_j , \\mu_j, \\Sigma_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\log \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^d}} + \\log \\exp(\\frac{-1}{2\\sigma^2} \\| x - \\mu_j^{(t-1)}\\|_2^2 )\n",
    "$$\n",
    "\n",
    "We can further remove positive constant multipliers and terms that don’t include $\\mu_j^{(t-1)}$.\n",
    "\n",
    "$$\n",
    "=  -\\| x - \\mu_j^{(t-1)}\\|_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\delta Q(\\mu_j^{(t-1)})}{\\delta \\mu_j^{(t-1)}} = 2 ( x - \\mu_j^{(t-1)}) \n",
    "$$\n",
    "\n",
    "Therfore\n",
    "\n",
    "$$\n",
    "\\mu_j^{(t)} = \\mu_j^{(t-1)} + \\eta' ( x - \\mu_j^{(t-1)}) \n",
    "$$\n",
    "\n",
    "We see how this recovers the online K-means algorithm. Now we move away from Gaussians and propose a differte instance of the online EM using a Poission mixture instead, which we then finally use to design an Soft-WTA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we chose the likelihood of the data $P(x | z, \\theta)$ to be Gaussian but this was simply to show the equivalence with the K-means algorithm. It might make more sense to use a Poisson likelihood since we would like individual values of the $^nx$ vectors (denoted $^nx_i$ in in the context of WTA-ANNS) to represent spiking rates. This makes sense since a Poisson RV represents the number of ocurrences or a random event withing a specific time. Furthermore, in a normal distributions there is always a posibility that a number drawn from such distribution is negative which would not make sense in this context. \n",
    "\n",
    "#### E step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma_j(^nx) = P (z_j | ^nx, \\theta) = \\frac{P(^nx|z_j, \\theta) P(z_j | \\theta)}{\\sum_k P(^nx|\\theta, z_k) P(z_k|\\theta)}\n",
    "$$\n",
    "\n",
    "We know from before that $P(z_j|\\theta) = w_j$, and that $P(^nx|\\theta, z_j) = \\prod_{i=1}^M \\mathcal{Pois}(^nx_i; \\lambda_{ji})$, where $\\lambda_{ji}$ denotes the rate parameter of component $j$ from element $i$ of the $n^{th}$ input. Note that this assumes that that the input from each neuron is generated form an independent Poisson, when in reality there might be stong correlation between the activities $x_i$. Perhaps there is a more convenient way of expressing this likelihood as a multivariate Poisson (TODO cite multivariate Poission).\n",
    "\n",
    "$$\n",
    "\\mathcal{Pois}(x_i; \\lambda_{ji}) = Pr(X=x_i) = \\frac{\\lambda_{ji}^{x_i} e^{-\\lambda_{ji}}}{x_i!}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma_j(^nx_i) = \\frac{w_j \\prod_{i=1}^M \\mathcal{Pois}(^nx_i; \\lambda_{ji})}{\\sum_{k=1}^K w_k \\prod_{i=1}^M \\mathcal{Pois}(^nx_i; \\lambda_{ki})}\n",
    "$$\n",
    "\n",
    "$$\n",
    " = \\frac{w_j \\prod_{i=1}^M \\lambda_{ji}^{x_i} e^{-\\lambda_{ji}}}{\\sum_{k=1}^K w_k \\prod_{i=1}^M \\lambda_{ki}^{x_i} e^{-\\lambda_{ki}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M step\n",
    "For a full derivation of the steps above we refere the reader to TODO: cite the Poisson EM.\n",
    "\n",
    "AQUI NOS QUEDAMOS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lambda_{ji} = \\frac{\\sum_{n=1}^N \\gamma_j(n^x_i) ^nx_i}{\\sum_{n=1}^n \\gamma_j(^nx_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_j = \\frac{\\sum_{n=1}^N \\gamma_j(^nx_i)}{n}\n",
    "$$\n",
    "\n",
    "We note thas as in the constrained version of the Hard EM, if we consider the $w_j$'s to be fixed we need only update the centroids $\\lambda_{j:}$, which we know the WTA-ANN is able to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-WTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that by showing that a WTA is equivalent to performing an online version of the EM algorithm we also show that a specific type of WTA shares the same thoretical assurances, namely that algorithm monotonically increases the lower bound of the likelihood.\n",
    "\n",
    "But perhaps even more importantly, it gives us a probabilistic framework which we can use to reason about the lerning. Specifically, the mixture weights $w_j = P(z=j | \\theta)$ can be oguht of as the overall poribability that a data point belongs to any given component. Here we propose that such probability can be seen as the homeostatic neuronal activity of the output neurons, e.g. the probability that a neuron will get activated upon an input. While our model does not learn this parameters as it is unclear how their update could be incorporated into the model, one can set them a priori as hyperparameters, and see how they affect the learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Standard WTA updates the weighst of only one of the output neurons, the one with the closest Eucledian distance from the input $x_i$. Here we aim to update every neuron accroding to the responsabiliy they have over a given data point. Specifically we weight the distance from a data point $x_i$ to the weights $\\lambda_{j:}$ asociated with cluster $m$ by the responsability $\\gamma_m(x_i)$ of clsuter $m$ over data point $x_i$.\n",
    "\n",
    "$$\n",
    "\\epsilon(x_i - \\lambda_{j:}) = \\frac{\\gamma_m(x_i)}{2} ||x_i - \\lambda_{j:}||_2^2 \n",
    "$$\n",
    "\n",
    "In the batch version of WTA we would have to account for all $N$ data points that are presented to the network and sum the distances weighted by the correspondign responsabilities.\n",
    "\n",
    "$$\n",
    "\\epsilon(X - \\lambda_{j:}) = \\frac{1}{2} \\sum_{i=1}^N \\gamma_m(x_i) ||x_i - \\lambda_{j:}||_2^2 \n",
    "$$\n",
    "\n",
    "By applying the derivative, setting it to zero and solving for $\\lambda_{j:}$ we get the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\epsilon'(X - \\lambda_{j:}) = - \\sum_{i=1}^N \\gamma_m(x_i) (x_i - \\lambda_{j:})\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^N \\gamma_m(x_i) \\lambda_{j:} - \\sum_{i=1}^N \\gamma_m(x_i) x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_{j:} = \\frac{\\sum_{i=1}^N \\gamma_m(x_i) x_i}{\\sum_{i=1}^N \\gamma_m(x_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can see that weighting the the error by the responsability $\\gamma_m(x_i)$ we recover the Soft EM update for the $\\lambda_{j:}$ of the clusters (here $w_:m$). In order to obtain the online version of the Soft-WTA, we follow the same strategy as before, namely we take the old estimate for the weights and we take a step in the oposite direction of the gradient of the error $\\epsilon(x_i - \\lambda_{j:})$, but this time weighted by $\\gamma_m(x_i)$.\n",
    "\n",
    "$$\n",
    "\\lambda_{j:} \\leftarrow \\lambda_{j:} - \\eta\\epsilon'(x_i - \\lambda_{j:})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon'(x_i - \\lambda_{j:}) = -\\gamma_m(x_i) (x_i - \\lambda_{j:}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda_{j:} \\leftarrow \\lambda_{j:} + \\eta\\gamma_m(x_i) (x_i - \\lambda_{j:}) \n",
    "$$\n",
    "\n",
    "TODO: note saying that we are aware that this does not fomrlaly show that there is an equivalnce betweeen the EM and the proposed algorithm but perhaps its a start? Perhaps we need to show that the gradient decent maximizes the likelihood function that we defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Assuming that this update is equivalent to the update performed in the M step of the EM algortihm, we are left with figuering out how our system could perform the E-step, namely, updateing the responsabilities of each cluster over a givendatapoint. If we view $\\gamma_m(x_i)$ as the normalized output of neuron $z_j$, it is only natural that the intensity with which the output neuron reacts to a stimuly affects the strenght of the learning, that is the magnitude of the change in the synaptic weights.\n",
    "\n",
    "Naturally this is where the lateral inhibiton of the ouput neurons comes in, such that the overall activity is a function of the activity of all of the nuerons in the output layer. This division operation could be implemented approximately with shunting inhibition of example, where the other neurons in the output layer set the conducntance such that the gain and the sumation dynamics of nenuon $z_j$ are is determined by the activity of all other neurons (TODO: cite divisin papaer here). This is due to the fact that the output volatege of a postsynaptioc neuron can be modeled as being inversly porportional to the conductance (set by lateral shuntting inhibition) and directly proportional to the driving current resulting from the temporal and spatial sumation of inhibitory and exitatory inputs comming from the input layer. Alternative ways of activity normalization have been proposed including synaptic sacleing and feedforward inhibition. It is not clear how exactly a biological circuit would implement exactly the operation for the updates of $\\gamma_j(x_i)$.\n",
    "\n",
    "Furthermore is makes sense that if we think of the mixing weights (TODO: referecen equation with the mixing weights) as the homsteatic activity level of output neuron $y_i$, that they are taken into account for the computation of its normalized reaction to an inoput $\\gamma_m(x_i)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO later:\n",
    "- Explore why Gaussians are a convininet ditribution to chooose ina our mixture models\n",
    "    - some ideas, cause of tuning curvs\n",
    "    - caveat, posibly some negative value which is not ok since fiering rate must be positive\n",
    "- adjust introduction\n",
    "- ok, maybe restructure everything to simply include the probabilistic model (with the exponential distribution) and then introduce the WTA model that will somoehow be analogous to it. \n",
    "\n",
    "Note on Neuromorphic circuits:\n",
    "\n",
    "* Analog subthershold circuits are used becasue they emmulate the diffusion of ions through the membrane in a neuron. This circuits are very noisy since a small change in the input wil gratly affect the output, which emulates the stochasticity in the brian.\n",
    "* Open question does the sigmoid can someohow approximate the integration of the both alteral inhibitoon from the output layer and also the direct exitation from the input layer, such that the averall outcome the activation of the nueron and such activation is the one mediating the hebian leraning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literature:\n",
    "\n",
    "+ [Feedforward Inhibition and Synaptic Scaling – Two Sides of the Same Coin?](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002432)\n",
    "+ [Normalization as a canonical neural computation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3273486/#R45)\n",
    "+ [Summation and Division by Neurons in Primate Visual Cortex](https://www.jstor.org/stable/2884085)\n",
    "+ [Homeostatic Regulation of Neuronal Excitability](http://www.scholarpedia.org/article/Homeostatic_Regulation_of_Neuronal_Excitability)\n",
    "+ [Input normalization by global feedforward inhibition expands cortical dynamic range](https://www.nature.com/articles/nn.2441)\n",
    "+ [Lognormal firing rate distribution reveals prominent fluctuation–driven regime in spinal motor networks](https://elifesciences.org/articles/18805#abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ [Expectation-Maximization for Estimating\n",
    "Parameters for a Mixture of Poissons](https://www.cs.helsinki.fi/u/bmmalone/probabilistic-models-spring-2014/mixture-of-poissons.pdf)\n",
    "+ [Gradient Ascent](https://web.stanford.edu/class/archive/cs/cs109/cs109.1192/lectureNotes/22%20-%20GradientAscent.pdf)\n",
    "+ [A review of multivariate\n",
    "distributions for count data derived\n",
    "from the Poisson distribution](https://www.cs.cmu.edu/~pradeepr/paperz/wics1398.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "+ How do neurons encode the covariance of the inoputs? Covariance is, given a component of the mixture, the information of how one of the coordinates in the random vecotr varies when as anothe relement vaires. Given that the synaptic weights are the menas of the Gaussian components, how are the covariances of these components encoded and updated?\n",
    "+ Is it ok to leave the weights (size of the components) as a hyper parameter set by the nature of the neuron population? Should individual neurons change this? How? Maybe with some kind of running averge.\n",
    "+ Maybe the softmax in the neurons is like diferential pari circuit? Invesitgate this by watching lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines to change:\n",
    "\n",
    "+ data points indexed with subscript i\n",
    "+ weights and neurons indexed with subscript m and n\n",
    "+ we have to homogenize notation: \n",
    "    + weights in the WTA-ANN are w and then w is mixture weights. this must change\n",
    "    + datapoints are denoted x_i in the context of the EM but in the WTA-ANN they are X_i with elements of this vector as x_n. \n",
    "    + We have to homogenize the number of clusters, in WTA-ANN they are M but later they are denoted K_j\n",
    "    + also labels are in WTA-ANN y_i and in the EM they are z\n",
    "    + perhaps it would make more sense to first define the parameters in the general mixture model and then use those everywhere.\n",
    "+ correct everywhere on the M step of the description of the EM to empasiste that the uopdate is based on the expected lower bound\n",
    "+ make it so everytihng has an iteration index in the EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General scheme:\n",
    "* ok but you must be thinking this is all  abtch procesing which is not how WTA-ANN work. then we introduce gradiant acent as a form of perfroing online instead of batch.\n",
    "* we show that this recovers the original algoritm for WTA under a gaussina assumption.\n",
    "* we now change to possion and derive the standar EM then the online EM for it.\n",
    "* tada we have our algorithm and we get to codeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
