{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Neural Population with Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present work explores how one can model learning in neural populations as an insatnce of online Gaussian Mixture Model (GMM) learning. First we descirbe the Hard Expectation-Maximization (EM) algorithm, a common method to infere GMM's. We then show how by assuming sperichal varainces and uniform wights the Hard EM algorithm becomes one and the same with the K-means algorithm. Then we show how K-means can be seen as an istance of a Winner Takes it All (WTA) network. \n",
    "\n",
    "We then briefly describe why the EM algorithm is convenient to model the system at hand and go on to relax the simplifications made in the Hard EM algorithm to give way tho the Soft EM algorithm. Finally we touch on the implications that lifting these simplifications has on the analogue nerual model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMMs and the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden variable models seek to explain observed variables in terms of latent ones. The EM algorithm allowes us to do maximum likelihood estimation (MLE) or maximum a posteriori (MAP) estimation with models that have hidden random variables (RVs). \n",
    "\n",
    "The EM algorithm finds one of it's use cases is clasification problems, where one seeks to label unlabled data points. In this context the hidden variables are the underlying labels of the data points. If we assume that the distribution of the data given that we know the label (also referred to as cluster identity) is Gaussian, this gives way to GMMs. \n",
    "\n",
    "Hence, we can write the conditional porbability of a specific data point as a linear combination of a set of Gaussians. \n",
    "\n",
    "$$\n",
    "P(x \\mid \\mu, \\Sigma, w) = \\sum_{i=1}^{K} w_i \\mathcal{N}(x; \\mu_i, \\Sigma_i)\n",
    "$$\n",
    "\n",
    "Here $x$ is our data point, and $\\mu_i$ and $\\Sigma_i$ represent the mean and covariance matrix of cluster $i$.\n",
    "\n",
    "### The Hard EM \n",
    "The EM algorithm iterates until convergence. One iteration of the EM starts by performming an Expectations step (E-step) followed by a Maximization step (M-step). For the Hard EM in the classification setting the E-step classifies each data point as belonging to one of the clusters. We denote this correspondance as $z_i$, where $i$ indicates the index of a specific data point. In the following equations superscript $(t)$ indicates the iteration in the algoritm and $\\theta$ our model parameters, namely $\\mu_i$ and $\\Sigma_i$\n",
    "\n",
    "#### E-step\n",
    "$$\n",
    "z_i^{(t)} = \\underset{z}{\\operatorname{arg max}} P(z | x_i, \\theta^{(t-1)})\n",
    "$$\n",
    "\n",
    "We can decompose $P(z | x_i, \\theta^{(t-1)})$ using Bayes rule to get the follwoing expression.\n",
    "\n",
    "$$\n",
    "z_i^{(t)} = \\underset{z}{\\operatorname{arg max}} \\frac{1}{Z} P(z | \\theta^{(t-1)}) P(x_i | z, \\theta^{(t-1)}) \n",
    "$$\n",
    "\n",
    "$P(z | \\theta^{(t-1)})$ is the probabity of any data point belonging to cluster $z$. By assuming that each cluster has the same size, these probabilities are the same for all clusters (e.g. $\\frac{1}{K}$) and hence do not play a role in the maximization argument. $P(x_i | z, \\theta^{(t-1)})$ is the probability of our data point belonging to cluster $z$ given our past estimate of the parameteres $\\theta^{(t-1)}$, namely $\\mathcal{N}(x_i; z, \\theta^{(t-1)})$. $\\frac{1}{Z}$ is the normalization constant from Bayes rule which is also independen of $z$ and therefor can be omited in the maximization. These observations give way to the folowing expression.\n",
    "\n",
    "$$\n",
    "z_i^{(t)} = \\underset{z}{\\operatorname{arg max}} \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^d}} \\exp(\\frac{1}{2\\sigma^2} \\| x_i - \\mu_z^{(t-1)}\\|_2^2 )\n",
    "$$\n",
    "\n",
    "We can see again that the maximization is only over the squared 2-norm and we can aslo note that maximizing this quantity is equivalent to minimizing the 2-norm.\n",
    "\n",
    "$$\n",
    "z_i^{(t)} = \\underset{z}{\\operatorname{arg min}} \\| x_i - \\mu_z^{(t-1)} \\|_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### M-step\n",
    "Thanks to the estimates of the cluster membership computed in the E-step we now have a full \"labeled\" data set $D^{(t)} = \\{ (x_1, z_1^{(t)}), \\dots, (x_n, z_n^{(t)})\\}$ and can now do maximum likelihood estimation (MLE) on the following quantitiy. \n",
    "\n",
    "$$\n",
    "\\theta^{(t)} = \\underset{\\theta}{\\operatorname{arg max}} P(D^{(t)} | \\theta)\n",
    "$$\n",
    "\n",
    "First we note that we can rewrite $P(D^{(t)} | \\theta)$ as $P(x_{1:n}, z_{i:n} | \\theta)$ and then use the chain rule to write $P(x_{1:n} | z_{i:n}, \\theta) P(z_{i:n} | \\theta)$. Then by applying the log we get the following expression. \n",
    "\n",
    "$$\n",
    "LL(\\theta) = \\log P(x_{1:n}, z_{i:n} | \\theta) = \\log P(x_{1:n} | z_{i:n}, \\theta) + \\log P(z_{i:n} | \\theta)\n",
    "$$\n",
    "\n",
    "We know from the preovious derivation that $P(z_{i:n} | \\theta^{(t-1)})$ are the weights of the clusters and that $P(x_{1:n} | z_{i:n}, \\theta^{(t-1)})$ is the normal distribution with parameters $\\theta^{(t-1)}$. Then by assuimng that the data points are conditionally independent we arrive at the following expression.\n",
    "\n",
    "$$\n",
    "= \\sum_i^n \\log w_i - \\log \\frac{1}{\\sqrt{(2\\pi\\sigma^2)^d}} - \\frac{1}{2\\sigma^2} || x_i - \\mu_z ||_2^2 \n",
    "$$\n",
    "\n",
    "Finally we can apply perform MLE on this expression by applying the derivative and setting it to zero to find the expression for the update of our parameters. Since in this specific case we are only updating the means $\\mu_z$ and leaving both the weights and the variances untoched we only show the expresion for the fromer. \n",
    "\n",
    "$$\n",
    "\\mu_z^{(t)} = \\frac{\\sum_i^n \\mathbb{1} (z_i^{(t)} = z) x_i}{\\sum_i^n \\mathbb{1} (z_i^{(t)} = z)}\n",
    "$$\n",
    "\n",
    "We can now see that by considering fixed sperical variances and weights the Hard EM perfomres exactly the same operations as the K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### K-means algorithm\n",
    "##### Step 0\n",
    "Initialize parameters for each centroid. \n",
    "\n",
    "##### Step 1\n",
    "$$\n",
    "z_i^{(t)} = \\underset{z}{\\operatorname{arg min}} \\| x_i - \\mu_z\\|_2\n",
    "$$\n",
    "\n",
    "##### Step 2\n",
    "$$\n",
    "\\mu_z^{(t)} = \\frac{\\sum_i^n \\mathbb{1} (z_i^{(t)} = z) x_i}{\\sum_i^n \\mathbb{1} (z_i^{(t)} = z)}\n",
    "$$\n",
    "\n",
    "##### Step 3\n",
    "Repeat step 1 and 2 until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Soft EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems often encountered when using the vanilla K-means is that the outcome of the clustering (the final asignment of data points to clusters), depends highlt on initialization. It is common for some centorids to be asigned two or more clusters of data points, while other centroids are left without children datapoints and hence never get updated. Here we call this issue uneaven membership assignmetn. This is partly due to the \"hardness\" of the asignment step. By modeling centroids as being only partly responsable for a given data point, instead of belonging exlusively to one group, we can enhance performance. This is what the Soft EM does, it assignes soft assignments instead of hard ones. \n",
    "\n",
    "<img align=\"center\" src=\"./img/problems_kmeans.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E-step\n",
    "Now we seek to find the responsability $\\gamma_j(x_i)$ of each cluster on each data point $x_i$, or in other words the probability that data point $x_i$ belongs to cluster $j$. Here $\\theta$ encompases $\\Sigma, \\mu$ and $w$.\n",
    "\n",
    "$$\n",
    "\\gamma_j(x_i) = P (z = j | x_i, \\theta) = \\frac{P(z = j, x, \\theta)}{P(x, \\theta)} = \\frac{P(x|z = j, \\theta) P(z = j | \\theta)}{P(x| \\theta)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(x|\\theta) = \\sum_k P(x|\\theta, z = k) P(z = k|\\theta)\n",
    "$$\n",
    "\n",
    "We know from before that $P(z = j|\\theta) = w_j$, and that $P(x|\\theta, z = j) = \\mathcal{N}(x; \\mu_j, \\Sigma_j)$, hence we can write the following.\n",
    "\n",
    "$$\n",
    "\\gamma_j(x_i) = \\frac{w_j\\mathcal{N}(x; \\mu_j, \\Sigma_j)}{\\sum_k w_k \\mathcal{N}(x;\\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "Once we have all the $\\gamma$'s for each data point, we again have a fully annotated data set, namely labels, which are now the responsabilities, and the cordinates of the data points themselves. We can now move one to infere the parametrs $\\theta$ in the M-step.\n",
    "\n",
    "#### M-step\n",
    "By aplying MLE on $\\log P(x_{1:n}, z_{i:n} | \\theta)$ we can get closed form expression for the parameters $w_j, \\mu_j$ and $\\Sigma_j$, for every cluster $j$ (derivation not shown).\n",
    "\n",
    "$$\n",
    "w_j^{(t)} \\leftarrow \\frac{1}{n} \\sum_{i=1}^n \\gamma_j^{(t)}(x_i) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_j^{(t)} \\leftarrow \\frac{\\sum_{i=1}^n \\gamma_j^{(t)}(x_i)x_i}{\\sum_{i=1}^n \\gamma_j^{(t)}(x_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Sigma_j^{(t)} \\leftarrow \\frac{\\sum_{i=1}^n \\gamma_j^{(t)}(x_i)(x_i-\\mu_j^{(t)})(x_i-\\mu_j^{(t)})^T}{\\sum_{i=1}^n \\gamma_j^{(t)}(x_i)}\n",
    "$$\n",
    "\n",
    "#### Implications\n",
    "Techincially speaking one could also perform the Hard EM algorithm relaxing both the spherical covariance asusmtion and the fixed uniform weights asusmtion. This is equivalent to allowing the underlying cluster to have different sizes and shapes, which might already help dilute uneaven membership assignment. However, the Hard EM will greedly assign a data point to one of the cluster even thou the probability that it belongs to either of them is the same or almost the same. This may lead to poor performance specially when the clusters are overlaping. By using membership intead of hard assignments we account for this uncertainty and also help poorly intialized clusters to get updated, since even if they are far away fron data points, will still have a responsability over them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winner Takes it All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have shown that K-means is equivalent to a resticted version of the Hard EM algorithm. Now we show that it is also equivalent to a WTA network. Given a 2 layered, fully conected artificial neural network (ANN), a WTA network is one where given an input, only one neuron $y_i$ in the output layer will be activated.\n",
    "\n",
    "<img align=\"center\" src=\"./img/wta.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, given a series of inputs $X_i = (x_1, ..., x_n)$, we seek optimally classify the inputs using our $m$ output neurons. We train our network by adjusting the $w_{nm}$ weights in the network. The idea is that given an input pattern $X_i$, the  asociated weights $w_{m:}$ of winning neuron $y_m$ get adjusted such that, if the same or a similar input $X_i'$ is presented to the network then $y_m$ is again the winner, thereby succesfully classifying the input. Following this rationale it is only natural to adjust the weights $w_{m:}$ by performing gradient decent w.r.t some error that is a function of the difference between the weights $w_{m:} \\in \\mathbb{R}^n$ and $X_i \\in \\mathbb{R}^n$. Note that this imposes some restirction on the values that the $w_{mn}$'s can take, namely they must be in the same order of magnitude as the inputs $X_i$ themselves. To ensure this one can simply normalize both the weights, the outputs and the inputs.\n",
    "\n",
    "$$\n",
    "w_{m:} \\leftarrow w_{m:} - \\epsilon'(X_i - w_{m:})\n",
    "$$\n",
    "\n",
    "If we choose the squared L2 norm as our messure of distance we can take the derivative w.r.t $w_{m:}$ and use the result as the update function $\\epsilon'(X_i - w_{m:})$. One can also modify the learinign schema such that a learning rate $\\eta$ is incorporated as $\\eta \\epsilon'(X_i - w_{m:})$ to control the magnitude of the adjustents, however here we stick to the vanilla version ommiting such term. \n",
    "\n",
    "$$\n",
    "\\epsilon(X_i - w_{m:}) = \\frac{1}{2} ||X_i - w_{m:}||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d}{dw_{m:}} \\epsilon(X_i - w_{m:}) =  \\frac{d}{dw_{m:}} \\frac{1}{2} ||X_i - w_{m:}||_2^2 = -(X_i - w_{m:}) = \\epsilon'(X_i - w_{m:})\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{m:} \\leftarrow w_{m:} + (X_i - w_{m:})\n",
    "$$\n",
    "\n",
    "The result is an online version of the K-means algoritm, where upon an inpuit the closest centroid (here the weights $w_{m:}$), gets pulled towareds the cordiantes of the input data. A perfect equivalence with the K-means algorithm would require that all the inputs $X_i$ to be presented at once. In such case we would simply adjust our cost function to account for all $N$ data points associated to the output $y_m$. By minimizing this quantity (applying the derivative, seting it to zero and solving for $w_{m:}$) we find that the update is the same as in the K-means algoritm, showing the equivalence between these two simingly different clustering schemas. \n",
    "\n",
    "$$\n",
    "\\epsilon(X - w_{m:}) = \\frac{1}{2} \\sum_{i=1}^N ||X_i - w_{m:}||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{w_{m:}} = \\underset{w_{m:}}{\\operatorname{arg min}} \\frac{1}{2} \\sum_{i=1}^N ||X_i - w_{m:}||_2^2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{w_{m:}} = \\frac{\\sum_i^N X_i}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are WTA networks of biological interest?\n",
    "In a nutshell, becasue they provide a toy model for a common conectivity pattern in the brain cortex, where neurons compete with each other for activation through lateral inhibition. \n",
    "\n",
    "We can desgin an biological circuit of toy neurons analogus to a WTA ANN, where neurons change their synaptic weights according to Hebbian Learing. Furthermore, by discretising the activation of the output neurons in this toy biological circuit, such thay only one neuron gets activated given an input, one can recover the previously described WTA ANN, and thereby an online version of the K-means algorithm. The winnig neuron will therefore be the only one to adjuste the weights of the corresponding synapses, thereby makeing it more likely that upon a similar input the same output neuron will fire. It is important to note that in such a biological system the update of the weight happens locally in the sens that if neuron $y_m$ is activated the weight $w_{nm}$ will se strengthened by hebian learining if there was an input from neuron $n$. Even thou previosuly we considered all $w_{:m}$ weight associated with neuron $y_m$ to be updated as a whole in a vector form, we can also think of the update takeing place individually for each element of $w_{:m}$.\n",
    "\n",
    "A more realistic model would allow for the posibility that multiple output neurons get activated given an input, even when the system is tuned such that one output neuron almost always gets acitaved when recivering a specific input parrtern while the other output neurons seldomly react to such input. Here we propose a Soft-WTA implmentation, where we implement this this output felxibility thorough responsabilities which encode the probability that a speciofic neuron fires given a specific input pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards a Soft-WTA\n",
    "Standard WTA updates the weighst of only one of the output neurons, the one with the closes Eucledian distance from the input. Here we aim to update every neuron accroding to the responsabiliy they have over a given data point. Specifically we weight the distance from a data point $X_i$ to the weights $w_{m:}$ asociated with cluster $m$ by the responsability $\\gamma_m(X_i)$ of clsuter $m$ over data point $X_i$.\n",
    "\n",
    "$$\n",
    "\\epsilon(X_i - w_{m:}) = \\frac{\\gamma_m(X_i)}{2} ||X_i - w_{m:}||_2^2 \n",
    "$$\n",
    "\n",
    "In the batch version of WTA we would have to account for all $N$ data points that are presented to the network and summ the distances weighted by the correspondign responsabilities.\n",
    "\n",
    "$$\n",
    "\\epsilon(X - w_{m:}) = \\frac{1}{2} \\sum_{i=1}^N \\gamma_m(X_i) ||X_i - w_{m:}||_2^2 \n",
    "$$\n",
    "\n",
    "By applying the derivative, seting it to zero and solving for $w_{m:}$ we get the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\epsilon'(X - w_{m:}) = - \\sum_{i=1}^N \\gamma_m(X_i) (X_i - w_{m:})\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 = \\sum_{i=1}^N \\gamma_m(X_i) w_{m:} - \\sum_{i=1}^N \\gamma_m(X_i) X_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{m:} = \\frac{\\sum_{i=1}^N \\gamma_m(X_i) X_i}{\\sum_{i=1}^N \\gamma_m(X_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that weighting the the error by the responsability $\\gamma_m(X_i)$ of cluster $m$ over datapoint $X_i$, in the bach scenario recovers the Soft EM update for the mean of the clusters (here $w_:m$). In order to obtain the online version of the Soft-WTA, we follow the same strategy as before, namely we take the old estimate for the weights and we take a step in the oposite direction of the gradient of the error $\\epsilon(X_i - w_{m:})$.\n",
    "\n",
    "$$\n",
    "w_{m:} \\leftarrow w_{m:} - \\epsilon'(X_i - w_{m:})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon'(X_i - w_{m:}) = -\\gamma_m(X_i) (X_i - w_{m:}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "w_{m:} \\leftarrow w_{m:} + \\gamma_m(X_i) (X_i - w_{m:}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "+ Now we are left with the issue of who we update the responsability.... and the weights and the variances which account for the geometry of the input deistibution and the probability thata. specific patter belongsa to any of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO later:\n",
    "- join WTA with EM and explains why the probabilistic interpretationn provides some intution about how and why things might be working on the brain\n",
    "- Explore the KL divergence thing that the soft EM optimizes\n",
    "- Explore why Gaussians are a convininet ditribution to chooose ina our mixture models\n",
    "    - some ideas, cause of tuning curvs\n",
    "    - becasue gausians are closed under sumation and product?\n",
    "- Remeber to mention the locality of the update for the onlie update rule\n",
    "- Intorduction might need some readjustment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
